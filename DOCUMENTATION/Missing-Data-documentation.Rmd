---
title: "Impute Missing Data with BLIMP"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview  

In this method, an input file with missing data is prepped in R and then run through BLIMP, which estimates missing data using multiple imputation and generates an output data set in which all missing cells are replaced with imputed values.

This procedure assumes that data are missing from the input file due to the missing at random (MAR) mechanism. A discussion of missing data theory is beyond the scope of this document, and such knowledge is not needed to use the method described here. Enders (2010) provides a thorough treament of missing data methods and theory.

``` 
Enders, C. K. (2010). Applied Missing Data Analysis. New York: Guilford. 
``` 

The code assumes a typical RStudio project folder hierarchy, with `INPUT-FILES` and `OUTPUT-FILES` folders at the first level.

### Prepare data for BLIMP

Input data should formatted with cases in rows and test items in columns, with a person ID column on the far left. On the input file, items should be renamed as follows: i001, i002, i003, etc. If the input file includes items from different scales/subtests, with different prefixes in their names, those prefixes must be dropped and the items renamed with the `i001` consecutive nomenclature. Do not include any variables in the input file besides the person ID and the item variables.

Once data are in this format, run the next block of R code to restructure the input for BLIMP.

Here and throughout, certain token markers are employed to designate user-input values that vary by project:  

* `{TOKEN}`: any value or series of values  
* `{FILE-PATH}`  
* `{FILE-NAME}` 

###### VALID CODE TO RUN

```{r prep_BLIMP_input, eval=FALSE}
suppressMessages(library(here))
suppressMessages(library(tidyverse))

file_name <- c("{FILE-NAME}")

input_orig <- suppressMessages(read_csv(here(
  str_c("INPUT-FILES/", file_name, ".csv")
))) 

NA_count <- sum(is.na(input_orig))
NA_count

input_orig[is.na(input_orig)] <- 999

input_tall <- input_orig %>%
  pivot_longer(cols = -id,
               names_to = "item",
               values_to = "response") %>%
  mutate(across(item, ~ str_sub(., 2, 4)))

write_csv(input_tall,
          here(
            str_c(file_name, '-BLIMP-input.csv')
            ),
          col_names = F
)
```

###### COMMENTED SNIPPETS
The snippet below contains one user-programmable input paramater. For the token `{FILE-NAME}`, substitute the _prefix_ of input file name (i.e., the file name stripped of `.csv`). Then use `readr::read_csv()` to read the input file into `input_orig`. `here::here()` holds the file path to input file; within `here()`, `stringr::str_c()` concatenates a sequence of string elements into a single string representing the file path.
```{r prep_BLIMP_input, echo=4:8, eval=FALSE}
```
When data are read into R, missing values are coded `NA` (not available). The next snippet counts the `NA` cells across all items and persons in the input table. `is.na()` returns `TRUE` for each table cell containing `NA`. Because logical `TRUE` is equivalent to a numerical code of `1`, `sum()` returns the count of `NA` across all cells. `NA_count` prints this count to the console.
```{r prep_BLIMP_input, echo=10:11, eval=FALSE}
```
Recode the `NA` in the input to `999`, a missing value code typically used by BLIMP. Assign the value `999` to the subset of cells within `input_orig` for which the predicate (logical) expression `is.na(input_orig)` returns `TRUE`.
```{r prep_BLIMP_input, echo=13, eval=FALSE}
```
Recall that the structure of the input file is a row for each case, and a column for each item. The model to be processed by BLIMP requires a multi-level (nested) structure, in which the items and responses are nested within each person. The nested rows contain _name-value pairs_ of items and their associated responses. In the multi-level structure, each person (each unique value of `id`) has the same number of rows as the number of items in the input file. 

Within the set of rows that share an identical value for `id`, the left-right sequence of columns (item names) in the input file is represented in the `item` column, going down the rows. In the `response` column, the response for each item appears in the same row as the item name. The following snippet accomplishes this transformation, from a wide input table to a tall (long) data object.

We use `tidyr::pivot_longer()` to reshape the table. The argument `cols = -id` identifies the columns that are to be changed from wide to long format. Recall that `input_orig` contains only the `id` and the `item` columns. The `-` (minus) operator indicates that `id` is to be _excluded_ from the set of columns to be changed from wide to long. By excluding it in this way, `id` appears as the left-most column in the transformed table, and is "stretched" down the table, creating new duplicate rows for each value of `id`, such that there are the same number of rows for each value of `id` as the number of `item` columns in the input file. 

The argument `names_to = "item"` indicates that the _names_ of the name-value pairs created by `pivot_longer()` will be stored in a column named `item`. The argument `values_to = "response"` indicates that the _values_ of the name-value pairs will be stored in a column named `response`.

In the transformed table, the `item` column contains the `item` names from the input file, repeated anew for (nested within) each successive set of `id` rows. The `response` column contains the item responses for each specific pairing of `id` and `item` values.

To understand how the transformation works, consider the following "before" and "after" examples of a small table containing three cases with responses from a four-item test:

###### Before

ID            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

###### After

ID            | Item          | Response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

```{r prep_BLIMP_input, echo=15:18, eval=FALSE}
```
BLIMP requires the item names in the `item` column to be numbers, not character strings. We can use `dplyr::mutate()` to make this change. Within `mutate()`, we use `across()` to apply a function to a subset of columns. The first argument specifies that `item` is the column to be modified. The second argument uses the formula shorthand `~` to apply `stringr::str_sub()` to the `item` column. The dot shorthand `.` indicates that `str_sub()` will operate on the previously subsetted column, and `2, 4` indicates that `str_sub()` will extract the numerical portion of the item name (that is, the substring within the name starting at position `2` and ending at position `4`).
```{r prep_BLIMP_input, echo = 19, eval = F}
```
We now use `readr::write_csv` to save the input file for BLIMP as a `.csv`, stripping the column names `col_names = F` as required by BLIMP. The input file is written to the top folder of the RStudio project. The BLIMP
script must also be located in this folder (if it is not, a file path must be specified on the BLIMP `DATA` command).
```{r prep_BLIMP_input, echo = 21:24, eval=FALSE}
```

<br>

### Run BLIMP script on prepped data

After processing in R, the BLIMP input file has the following structure:

* Three columns: Person ID, item number, item response
* The structure is multi-level, in which item-response pairs are nested within each value of Person ID
* No column (variable) names

In simple terms, BLIMP accomplishes the imputation of missing values by estimating a series of linear regression models on the input data. The estimation is an iterative process, whereby BLIMP posits model parameters, derives regression equations, imputes missing data by adding random residual terms to the regression equations, and repeats the process using the newly-imputed data set. Iteration continues for a defined interval to allow regression coefficients to stabilize. The stable regression equations are then used to impute a final complete data set that can be used for downstream analysis.

The model estimation process must include a burn-in phase: a series of iterations that allow the regression parameters to converge (stabilize). The number of iterations required for burn-in varies by data set and is initially unknown. BLIMP supplies a metric for determining convergence: potential scale reduction (PSR). An acceptable burn-in interval is defined as the minimum number of iterations required to yield a PSR value in the range of 1.05-1.10.

The BLIMP imputation process consists of two steps:

1. __Diagnostic Run__: Data are processed with an initial burn-in interval of 1000 iterations. The output table of PSR values is examined, and if PSR is not reduced to 1.05-1.10 by the last set of iterations, the data are re-run with a longer burn-in interval (e.g. 2000 iterations).  This process continues until an acceptable number of burn-in iterations is determined.

2. __Imputation Run__: Data are processed using the burn-in interval determined in Step 1. The saved output data set has all missing responses replaced with imputed values.

An identical BLIMP template script can be used for Steps 1 and 2. This template consists of the following commands (some include values and arguments that remain constant for all input data sets):

```{r BLIMP-template, eval=FALSE}
DATA: {FILE-NAME}.csv
VARIABLES: {TOKEN}
ORDINAL: {TOKEN}
NOMINAL: {TOKEN}
FIXED: {TOKEN}
CLUSTERID: {TOKEN}
MISSING: 999
MODEL: {TOKEN}
SEED: 90291;
BURN: {TOKEN}
ITERATIONS:  1;
OPTIONS: psr;
SAVE: separate = {FILE-NAME}*.csv;
```

Steps 1 and 2 are differentiated by activation/deactivation of the `OPTIONS` and `SAVE` lines (commands can be toggled off by beginning the command line with `#`).

###### Step 1: Diagnostic Run
`OPTIONS` is activated to generate the PSR table. `SAVE` is deactivated because no imputed data set is needed for this step.
```{r BLIMP-multiFactor-template1, eval=FALSE}
OPTIONS: psr;
# SAVE: separate = {FILE-NAME}*.csv;
```
###### Step 2: Imputation Run
`OPTIONS` is deactivated because the burn-in interval has been determined by Step 1, so there is no need to view the PSR table. `SAVE` is activated in order to generate an imputed data set with missing values estimated.
```{r BLIMP-multiFactor-template2, eval=FALSE}
# OPTIONS: psr;
SAVE: separate = {FILE-NAME}*.csv;
```

Below is an example BLIMP script based on this template. The example has a typical set of token substitutions. The file names are generic - any suitable names can be substituted.

###### THIS CODE CAN BE RUN IN BLIMP STUDIO
```{r BLIMP_example, eval=FALSE}
DATA: BLIMP-input.csv;
VARIABLES: id itemnum response;
ORDINAL: response;
NOMINAL: itemnum;
FIXED: itemnum;
CLUSTERID: id;
MISSING: 999;
MODEL: response ~ itemnum;
SEED: 90291;
BURN: 2000; 
ITERATIONS: 1;
OPTIONS: psr;
SAVE: separate = BLIMP-output*.csv;
```

###### COMMENTS
In order to run as written, the `DATA` command must name a file located in the same folder/directory as the BLIMP script. If the input file is located elsewhere, a file path must be specified on `DATA`. Because the input date file has no column names, the `VARIABLES` command is used to name the three columns in the input file: `id`, `itemnum`, `response`. The number of names on the `VARIABLES` line must be identical to the number of columns in the input file.
```{r BLIMP_example, echo=1:2, eval=FALSE}
```
`ORDINAL` and `NOMINAL` are used to designate ordered and non-ordered categorical variables. Here, `response` is listed on the `ORDINAL` line. In the input data file, `response` refers to a 1-2-3-4 item response format; i.e., ordered categories representing increasing frequency of a behavior. On the `NOMINAL` line, `itemnum` represents a non-ordered categorical variable. Variables on the `NOMINAL` line are automatically recoded into dummy codes at imputation.
```{r BLIMP_example, echo=3:4, eval=FALSE}
```
`FIXED` is used to designate variables that will not have their means and variances estimated during computation. Fixing variables in this way increases computational efficiency. In this example, the means and variances of the nominal item number variable are not of substantive interest; thus, there is no reason to estimate these parameters. 

`CLUSTERID` designates a multilevel (nested) design. In this example, on the input file, name-value pairs of `itemnum` and `response` are nested within each value of `id`. `MISSING` identifies the missing data code (`999` was coded in the input file by the R script). 
```{r BLIMP_example, echo=5:7, eval=FALSE}
```
`MODEL` specifies the regression model that will be used for imputation. On this `MODEL` line, `response` is regressed on the dummy-coded item-number variable (dummy codes were created by listing `itemnum` on the `NOMINAL` command line). 

More specifically, the model being estimated is a two-level regression model, in which items are nested within persons. `itemnum` is a fixed level-1 predictor of the level-2 variable, `response`, and the `itemnum` variable itself is in fact a proxy for item difficulty (in Rasch terms). If there are _k_ items in the input file, BLIMP creates _k-1_ dummy code variables to represent the items.

BLIMP estimates a regression model that can be specified as follows:

Y~_ip_~ = {B~_0_~ + B~_1_~(D~_1ip_~) + â€¦ + B~_k-1_~(D~_k-1ip_~)} + U~_p_~ + E~_ip_~

The expression within the curly braces `{}` is the _fixed_ component of the model, and the rest of the terms constitute the _random_ component of the model. In addition:

 * Y~_ip_~: predicted response to item _i_ by person _p_
 * B~_0_~: fixed intercept, which BLIMP sets to the mean score of item 1 across persons
 * B~_1_~ ... B~_k-1_~: fixed slopes (one for each of the _k-1_ dummy variables), which BLIMP sets to the difference between the mean score of item 1 (i.e., B~_0_~) and the mean score of the item represented by a specific dummy code variable (e.g., B~_5_~ = mean(item 6) - mean(item 1)). These slopes capture item difficulty.
 * D~_1ip_~ ... D~_k-1ip_~: dummy code variables (fixed predictors in the model)
 * U~_p_~: random intercept term that captures the influence of person ability on Y~_ip_~, above and beyond the variance explained by item difficulty (i.e., the fixed component of the regression model)
 * E~_ip_~: random error at the level of individual item response values
 
 It is important to note that in this model, items can vary in difficulty relative to each other, but not across persons. This captures the meaning of a fixed slope. For example, the values of slopes B~_2_~ and B~_5_~ can differ, which reflects real differences in difficulty between the items to which these slopes refer. But the values of B~_2_~ and B~_5_~ do not vary across persons, just as item difficulty is expected to be stable for persons of varying ability.
 

```{r BLIMP_example, echo=8, eval=FALSE}
```
`SEED` sets BLIMP's random-number generator so that the final output is identical each time the script is run. `BURN` specifies the number of iterations that are run prior to saving the first imputed data set. As described above, `BURN` is initially set at 1000 for the diagnostic run, and this value is adjusted as needed to until PSR reaches the acceptable threshold. This adjusted value is then used for the imputation run.

`ITERATIONS` controls how many iterations are run after completing burn-in. For the present purpose, which is generating an imputed data set, `ITERATIONS` is set to 1. If the project required regression parameters as explicit output, `ITERATIONS` would be set to at least 1000 to ensure stable estimates.
```{r BLIMP_example, echo=9:11, eval=FALSE}
```
`OPTIONS` and `SAVE` are the two BLIMP commands that are set differently for the diagnostic and imputation runs. For the diagnostic run, `SAVE` is toggled off, and `OPTIONS` is run to generate the PSR table that is used to determine the optimal burn-in interval. For the imputation run, `OPTIONS` is toggled off, and `SAVE` is run to save the imputed data set. The `separate` argument indicates that each imputed data set is to be saved as a separate file (as opposed to being stacked on top of one another in a single file). The file name on `SAVE` requires an `*`, which allows BLIMP to append numerical suffixes on multiple imputed data files. (`*` is always used, even in this example, which yields only a single imputed data set)
```{r BLIMP_example, echo=12:13, eval=FALSE}
```

<br>

### Reformat imputed data set for downstream analysis

BLIMP writes the imputed data set as a `.csv`, in a tall format without column names. This R code restores the imputed data to its original input structure for subsequent analysis.

###### VALID CODE TO RUN

```{r reformat_impute, eval=FALSE}
impute_name <- c("{FILE-NAME}")
impute_path <- c("{FILE-PATH}")

temp1 <- suppressMessages(
  read_csv(
    here(
    str_c(impute_path, impute_name, ".csv")
  ),
    col_names = F))
names(temp1) <- c("id", "item", "response")
temp2 <- temp1 %>% 
  pivot_wider(
    id_cols = id,
    names_from = item,
    values_from = response
  )
names(temp2) <- names(input_orig)

NA_count <- sum(temp2 == 999)
NA_count

write_csv(temp2, here(
  str_c(
    'OUTPUT-FILES/',
    file_name,
    '-noMiss.csv'
  )
))
```

<br>

###### COMMENTED SNIPPETS
The imputed data file `BLIMP-output1.csv` has no column names, so `read_csv()` must be called with the `col_names = F` argument (otherwise, the first row of data will be used as column names). The imputed data file is read into an R object called `temp1`. Column names are applied to `temp1` from a character vector using `base::names()`.
```{r reformat_impute, echo=1:10, eval=FALSE}
```
The data object `temp1` is a tall table in which `item`s and their associated `response`s are nested within each person `id` number, as in the following example:

###### Before

id            | item          | response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

To restore this data object to a wide format, we call `tidyr::pivot_wider()`. Its first argument `id_cols = id` collapses the multiple duplicate rows of `id` into a single row for each `id` number. `names_from = item` indicates that the values of the `item` column are used to name new columns going left-to-right across the transformed table. `values_from = response` indicates that values from the `response` column are used to fill the cells of the newly restored item columns. In the restructured table, each `id` row holds all of the item responses for a single person, as in the original input file.

The transformed table appears as in the example below:

###### After

id            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

Thus, `pivot_wider` restores the imputed data set to the same format as original input (which contained missing data). In this sense, `pivot_wider` is the inverse of `pivot_longer`, which was used in earlier code to transform the original input into the multi-level (nested) format required by BLIMP.

In the remainder of this code chunk, `names()` is used to reapply the column names of the original input. The code verifies that the count of `NA` (now coded as `999`) is 0, and writes the final output to `.csv`. The output has no missing data and is thus ready for downstream analysis.

```{r reformat_impute, echo=11:28, eval=FALSE}
```
