---
title: "Impute Missing Data with BLIMP"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview  

This documents a method in which input data are prepped in R and then run through BLIMP, which estimates a model and generates an output data set in which all missing cells are replaced with imputed values.

The code is flexible: it can handle any number of items, subscales, and item-scale assignments.

Code assumes a typical RStudio project folder hierarchy, with `INPUT-FILES` and `OUTPUT-FILES` folders at the first level.

### Prepare data for BLIMP

Input data should formatted with cases in rows and items in columns with a person ID column on the far left. On the input file, items should be renamed as follows: i001, i002, i003, etc. If the input file includes items from different scales/subtests, with different prefixes in their names, those prefixes must be dropped and the items renamed in the consecutive sequence described in the previous sentence. The code then reassigns them to their scales/subtests as it reformats the input file.

Run the following code to restructure the input for BLIMP.

Here and throughout, certain token markers are employed to designate user-input values that vary by project:  

* `{TOKEN}`: any value or series of values  
* `{FILE-PATH}`  
* `{FILE-NAME}` 

###### VALID CODE TO RUN

```{r prep_BLIMP_input, eval=FALSE}
suppressMessages(library(here))
suppressMessages(library(tidyverse))
suppressMessages(library(recipes))

id <- c('{TOKEN}')
first_item <- c('{TOKEN}')
last_item <- c('{TOKEN}')
file_name <- c('{FILE-NAME}')
scale_assign <- list({TOKEN})
scale_num <- {TOKEN}
names(scale_assign) <- scale_num

input_orig <- suppressMessages(read_csv(here(
  paste0('INPUT-FILES/', file_name, '.csv')
))) %>% 
  select(id, first_item:last_item)

NA_count <- input_orig %>% summarise_all(list(~sum(is.na(.)))) %>%
  mutate(
    all_NA = rowSums(.[2:ncol(.)]))

input_orig[is.na(input_orig)] <- 999

input_gathered <- input_orig %>%
  gather('item','response',-id) %>% 
  group_by(id) %>% 
  arrange(id) %>% 
  mutate(item = as.integer(str_sub(item, 2, 4)))

input_scale <- enframe(scale_assign) %>%
  unnest(item = value) %>%
  rename (scale = name) %>%
  mutate_all(as.integer) %>%
  right_join(input_gathered, by = "item") %>%
  select(id, item, response, scale) %>%
  mutate(scale_last = case_when(scale != lead(scale) |
                                  is.na(lead(scale)) ~ 1,
                                TRUE ~ NA_real_)) %>%
  mutate_at(vars(item), ~ as.factor(.))

dum <- input_scale %>% 
  recipe(~ .) %>% 
  step_dummy(item, one_hot = T, preserve = T) %>% 
  prep(training = input_scale) %>%
  bake(new_data = input_scale) %>% 
  mutate_at(vars(starts_with("item_")), ~replace(., scale_last == 1, 0)) %>% 
  select(-scale_last)

write_csv(dum,
          here(paste0(file_name, '-BLIMP-input.csv')),
          col_names = F
)
```

###### COMMENTED SNIPPETS
The snippet below contains user-programmable input paramaters. Here are the token substitutions:

* `id`: name of person ID column
* `first_item`: name of column holding first item
* `last_item`: name of column holding last item
* `file_name`: _prefix_ of input file name (e.g., stripped of `.csv`)
* `scale_assign`: this list contains a series of integer ranges (e.g., `1:14`, `15:33`, `34:48`, etc.) that group items by their assigned subscale. For example, the range `1:14` gives the numbers of the 14 items that comprise subscale 1.
* `scale_num`: number of subscales, expressed as an interger range starting with 1 (e.g., `1:8`)

```{r prep_BLIMP_input, echo=5:11, eval=FALSE}
```
Read the input table into `input_orig`. Keep only the `id` and `item` columns.
```{r prep_BLIMP_input, echo=13:16, eval=FALSE}
```
Count the `NA` cells across all items and persons in the input table. `dplyr::summarise_all` applies a summary function across all columns in the input table. In this instance, `base::is.na()` is the kernel function, which returns a logical `TRUE` for each `NA` cell (the `.` shorthand indicates the function is being applied to the piped data object). This is wrapped in `~sum()`, which returns a sum of the `TRUE`s for each column. This complete expression is passed to `summarise_all()` within a `list()`, which allows existing variables to be modified in-place by the summary function. The end result is a one-row table containing the counts of `NA` within each item column, retaining the original column names. `mutate()` is used to create the new column `all_NA`, which is the sum `base:rowSums` of the `NA` across all item columns. The columns to be summed are captured by the expression `.[2:ncol(.)]`, which refers to the piped data object `.`, subsetted by columns `[]`, including the 2nd through the last column (indicated by `ncol(.)`, the number of columns in the piped object).
```{r prep_BLIMP_input, echo=18:20, eval=FALSE}
```
Recode the `NA` in the input to `999`, the BLIMP prefered missing value code. Assign the value `999` to the subset of cells within `input_orig` for which the predicate (logical) expression `is.na(input_orig)` returns `TRUE`.
```{r prep_BLIMP_input, echo=22, eval=FALSE}
```
Recall that the structure of the input file is a row for each case, and a column for each item. The model to be processed by BLIMP requires a multi-level (nested) structure, in which the items and responses are nested within each person. The nested rows contain key-value pairs of item-response. Thus each person has the same number of rows as the number of items. Within each set of person-rows, the left-right sequence of columns (item names) in the input file is represented in the `item` column, going down the rows. In the `response` column, the response for each item appears in the same row as the item name. The following snippet accomplishes this transformation, from a wide (or spread) input table, to a tall (or gathered) piped data object.

```{r prep_BLIMP_input, echo=24:28, eval=FALSE}
```


```{r read_input1, eval=FALSE}
 
# add column that labels the row containing the last item in each scale/subtest.
# This labeling is needed for BLIMP dummy coding, in which scale items are
# dummy-coded relative to the last item in their assigned scale.
input_gathered <- input_gathered %>% mutate(
  scale_last = case_when(
    scale != lead(scale) | is.na(lead(scale)) ~ 1,
    TRUE ~ NA_real_
  )
)

# use `recipes::step_dummy` to create the dummy codes.
dum <- input_gathered %>% 
  recipe(~ .) %>% 
  step_dummy(item, one_hot = T, preserve = T) %>% 
  prep(training = input_gathered) %>%
  bake(new_data = input_gathered) %>% 
  mutate_at(vars(starts_with("item_")), ~replace(., scale_last == 1, 0)) %>% 
  select(-scale_last)

# write the input file for BLIMP, stripping the column names as required. The
# input file is written to the top folder of the RStudio project. The BLIMP
# script must also be located in this folder
write_csv(dum, here('BLIMP-input.csv'), col_names = F)
```

<br>

### Run BLIMP script on prepped data

After processing in R, the input data has the following structure:

* Left-most four columns are PersonID, item number, item response, numerical label for item-scale assignment
* Al columns to the right of these four are dummy codes, in which each scale item is dummy coded relative to the last item in that scale

###### THIS CODE CAN BE RUN IN BLIMP STUDIO
```{r BLIMP, eval=FALSE}
DATA: BLIMP-input.csv;
VARIABLES: id itemnum response scalenum s1d1-s1d14 s2d1-s2d16 s3d1-s3d12 s4d1-s4d23 s5d1-s5d8 s6d1-s6d14 s7d1-s7d16 s8d1-s8d11;
ORDINAL: response;
NOMINAL: scalenum;
FIXED: scalenum s1d1-s1d13 s2d1-s2d15 s3d1-s3d11 s4d1-s4d22 s5d1-s5d7 s6d1-s6d13 s7d1-s7d15 s8d1-s8d10;
CLUSTERID: id;
MISSING: 999;
MODEL: response ~ scalenum s1d1-s1d13 s2d1-s2d15 s3d1-s3d11 s4d1-s4d22 s5d1-s5d7 s6d1-s6d13 s7d1-s7d15 s8d1-s8d10 | scalenum;
SEED: 90291;
BURN: 1000; 
ITERATIONS: 1000;
OPTIONS: estimates;
# CHAINS: 4 processors 4;
SAVE: separate = model4imp*.csv;
```

<br>

###### COMMENTED SNIPPETS
`VARIABLE` names to the right of `scalenum` are the dummy codes (d) for each of eight scales (s).
```{r BLIMP, echo=1:2, eval=FALSE}
```
Variables on `NOMINAL` line are recoded into dummy codes at imputation.
```{r BLIMP, echo=3:4, eval=FALSE}
```
`CLUSTERID` designates a multilevel (nested) design. In this example, on the input file, values of `itemnum` are nested within each value of 'id'.
```{r BLIMP, echo=5:6, eval=FALSE}
```
`MISSING` designates the missing data code. In this `MODEL`, response is regressed on the dummy-coded scale variables (created by `NOMINAL` command) and the dummy-coded item item variables (represented in columns in the input .csv). 

The expression `| scalenum` allows the model to estimate individual differences for each scale. This is a flexible model in which the only assumption is that within-scale
factor loadings (thetas) are identical. Items within scales can have unique means, and thetas can vary across scales and between persons.
```{r BLIMP, echo=7:8, eval=FALSE}
```
Set `SEED` so that output is identical on each run. `BURN` runs 1000 iterations before saving the first imputed data set, to allow parameter estimates to stabilize. Number of `BURN` iterations needs to be recalibrated for each new data set by examining psr, plots. `ITERATIONS` conrols how many iterations are run after completing burn-in, set this to at least 1000 to ensure stable parameter estimates. 
```{r BLIMP, echo=9:11, eval=FALSE}
```
`OPTIONS` requests parameter estimates for the analysis output. `CHAINS` is toggled off here, but it can be turned on for parallel (and potentially faster) processing. In this example, four chains would then be run, resulting in four imputed data sets. `SAVE: separate` indicates that imputations will be saved in separate files (this script is set up to yield only one imputed data set). The asterisk `*` is required so that BLIMP can distinguish multiple separate imputations with a numerical suffix.
```{r BLIMP, echo=12:14, eval=FALSE}
```

<br>

### Reformat imputed data set for downstream analysis

The imputed data set is in a tall (gathered) format with the dummy code columns. This code restores the data to its original input structure and writes an output file.

```{r reformat_impute, eval=FALSE}
spm_p_noMiss <- suppressMessages(
  read_csv(
    (here('model4imp1.csv')), 
  col_names = c('id', 'item', 'response'))[1:3]) %>% 
  spread(item, response)
names(spm_p_noMiss) <- names_input

# Count NA across all columns
NA_count_noMiss <- spm_p_noMiss %>% summarise_all(list(~sum(is.na(.)))) %>%
  mutate(
    all_NA = rowSums(.[2:115]))
NA_count_noMiss$all_NA

# Write output
write_csv(spm_p_noMiss, here('OUTPUT-FILES/SPM-P-noMiss.csv'))
```

<br>

### Annotated BLIMP output
For all output tables, the entries in the mean/StdDev columns can be regarded as Bayesian point estimtes and standard errors. The model allows each person to have a latent scale (subscale) score on each subscale/factor/dimension. These latent scores are approximately scaled as z-scores.

#### Variances Section

* Lines that begin with L2 and have only a single quantity in the `Variances` column (e.g., `L2 Intercept (i)`, `L2 scalenum#2`) are _variances_ of a particular scale score. `L2 Intercept (i)` is the variance of scale #1. 
* Lines that have two elements (e.g., `L2 scalenum#3, scalenum#2`) are the _covariances_ between two scale scores.

```{r BLIMP_output_var, eval=FALSE}
ANALYSIS MODEL ESTIMATES:


Missing outcome: response   

                                --------------------------------------------------------
Parameters                      |   Mean   |  Median  |  StdDev  |Lower 2.5 |Upper 97.5|
                                --------------------------------------------------------
Variances:                      |          |          |          |          |          |
  L2 Intercept (i)              |     0.729|     0.707|     0.062|     0.675|     0.860|
  L2 (i), scalenum#2            |    -0.365|    -0.344|     0.059|    -0.474|    -0.295|
  L2 scalenum#2                 |     0.678|     0.685|     0.062|     0.602|     0.794|
  L2 (i), scalenum#3            |    -0.400|    -0.378|     0.064|    -0.523|    -0.333|
  L2 scalenum#3, scalenum#2     |     0.670|     0.654|     0.069|     0.595|     0.820|
  L2 scalenum#3                 |     0.836|     0.803|     0.088|     0.759|     1.039|
  L2 (i), scalenum#4            |    -0.371|    -0.332|     0.066|    -0.502|    -0.307|
  L2 scalenum#4, scalenum#2     |     0.538|     0.530|     0.059|     0.469|     0.667|
  L2 scalenum#4, scalenum#3     |     0.647|     0.615|     0.073|     0.592|     0.827|
  L2 scalenum#4                 |     0.704|     0.690|     0.061|     0.642|     0.840|
  
  [OUTPUT TRUNCATED FOR BREVITY]
  
  Residual Var.                 |     1.000|     1.000|     0.000|     1.000|     1.000|
                                |----------|----------|----------|----------|----------|
```

#### Coefficients Section

* `Intercept`: grand mean of latent scale #1.
* `scalenum`: dummy codes reflecting mean differences of the listed scale from scale #1 (expressed roughly on a z-score scale). 
    + In this example, all scales have lower means than scale #1, except for scale #3. Confidence intervals in the right-most columns tell whether a scale's mean differs significantly from the mean of scale #1.
* `s1d1` et al.: dummy codes that capture differences among items with respect to difficulty.
    + Coefficents give mean difference of each item relative to last item on that scale.

```{r BLIMP_output_coeff, eval=FALSE}
Coefficients:                   |          |          |          |          |          |
  Intercept                     |     0.136|     0.119|     0.053|     0.078|     0.231|
  scalenum#2                    |    -0.507|    -0.498|     0.056|    -0.623|    -0.421|
  scalenum#3                    |     0.295|     0.321|     0.077|     0.152|     0.374|
  scalenum#4                    |    -1.085|    -1.057|     0.083|    -1.255|    -1.003|
  scalenum#5                    |    -1.979|    -1.968|     0.099|    -2.131|    -1.864|
  scalenum#6                    |    -1.784|    -1.798|     0.056|    -1.854|    -1.707|
  scalenum#7                    |    -1.163|    -1.162|     0.047|    -1.236|    -1.077|
  scalenum#8                    |    -2.004|    -1.987|     0.082|    -2.169|    -1.910|
  s1d1                          |     0.003|     0.016|     0.079|    -0.122|     0.098|
  s1d2                          |     0.046|     0.062|     0.069|    -0.055|     0.130|
  s1d3                          |     0.307|     0.305|     0.057|     0.203|     0.409|

      [OUTPUT TRUNCATED FOR BREVITY]

  s8d7                          |     0.660|     0.637|     0.073|     0.536|     0.765|
  s8d8                          |     0.775|     0.761|     0.045|     0.704|     0.838|
  s8d9                          |     1.289|     1.282|     0.062|     1.190|     1.391|
  s8d10                         |     1.302|     1.322|     0.053|     1.183|     1.356|
                                |----------|----------|----------|----------|----------|
```

#### Threshold Section

* `Tau`: set of z-score cutpoints that slice underlying latent normal scores into four discrete response options (for four-point item rating scale). This is not a direct analog to polytomous response thresholds in IRT, because the latter are available on an item-by-item basis, whereas these Tau paramaters represent a shared aspect of all items in the BLIMP analysis.

```{r BLIMP_output_tau, eval=FALSE}
Thresholds:                     |          |          |          |          |          |
  Tau 1                         |     0.000|     0.000|     0.000|     0.000|     0.000|
  Tau 2                         |     1.598|     1.598|     0.005|     1.590|     1.606|
  Tau 3                         |     2.555|     2.556|     0.014|     2.532|     2.572|
                                |----------|----------|----------|----------|----------|
```


