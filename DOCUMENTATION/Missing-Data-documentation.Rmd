---
title: "Impute Missing Data with BLIMP"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview  

This documentation covers a method in which input data are prepped in R and then run through BLIMP, which estimates a model and generates an output data set in which all missing cells are replaced with imputed values.

The method can handle two general types of input data:

* __Multi-factor model__: the test has a factor structure in which items are assigned to multiple subscales.
* __Single-factor model__: the test has a single factor/scale structure.

The multi-factor code is flexible: it can handle any number of items, subscales, and item-scale assignments. The single-factor code is a simplified subset of the multi-factor code.

The code assumes a typical RStudio project folder hierarchy, with `INPUT-FILES` and `OUTPUT-FILES` folders at the first level.

### Prepare data for BLIMP: Multi-factor model

Input data should formatted with cases in rows and items in columns with a person ID column on the far left. On the input file, items should be renamed as follows: i001, i002, i003, etc. If the input file includes items from different scales/subtests, with different prefixes in their names, those prefixes must be dropped and the items renamed with the `i001` consecutive nomenclature. The code then reassigns them to their scales/subtests as it reformats the input file.

Once data are in this format, run the next block of code to restructure the input for BLIMP.

Here and throughout, certain token markers are employed to designate user-input values that vary by project:  

* `{TOKEN}`: any value or series of values  
* `{FILE-PATH}`  
* `{FILE-NAME}` 

###### VALID CODE TO RUN

```{r prep_BLIMP_input, eval=FALSE}
suppressMessages(library(here))
suppressMessages(library(tidyverse))
suppressMessages(library(fastDummies))

id <- c('{TOKEN}')
first_item <- c('{TOKEN}')
last_item <- c('{TOKEN}')
file_name <- c('{FILE-NAME}')
scale_assign <- list({TOKEN})
scale_num <- {TOKEN}
names(scale_assign) <- scale_num

input_orig <- suppressMessages(read_csv(here(
  paste0('INPUT-FILES/', file_name, '.csv')
))) %>% 
  select(id, first_item:last_item)
names_input_orig <- names(input_orig)

NA_count <- sum(is.na(input_orig))
NA_count

input_orig[is.na(input_orig)] <- 999

input_gathered <- input_orig %>%
  gather('item','response',-id) %>% 
  group_by(id) %>% 
  arrange(id) %>% 
  mutate(item = as.integer(str_sub(item, 2, 4)))

input_scale <- enframe(scale_assign) %>%
  unnest(item = value) %>%
  rename (scale = name) %>%
  right_join(input_gathered, by = "item") %>%
  select(id, item, response, scale) %>%
  mutate(scale_last = case_when(scale != lead(scale) |
                                  is.na(lead(scale)) ~ 1,
                                TRUE ~ NA_real_)) %>%
  mutate_at(vars(item), ~ as.factor(.))

dum <- input_scale %>% 
  dummy_cols(select_columns = 'item') %>% 
  mutate_at(vars(starts_with("item_")), ~replace(., scale_last == 1, 0)) %>% 
  select(-scale_last)

write_csv(dum,
          here(paste0(file_name, '-BLIMP-input.csv')),
          col_names = F
)
```

###### COMMENTED SNIPPETS
The snippet below contains user-programmable input paramaters. Here are the token substitutions:

* `id`: name of person ID column
* `first_item`: name of column holding first item
* `last_item`: name of column holding last item
* `file_name`: _prefix_ of input file name (e.g., stripped of `.csv`)
* `scale_assign`: this list contains a series of integer ranges (e.g., `1:14`, `15:33`, `34:48`, etc.) that group items by their assigned subscale. For example, the range `1:14` gives the numbers of the 14 items that comprise subscale 1.
* `scale_num`: numeric names for the subscales, expressed as an integer range starting with 1 (e.g., `1:8`)

`names(scale_assign) <- scale_num` merges `scale_assign` and `scale_num` by converting the `scale_assign` into a named list, using the values of `scale_num` as the names for the elements of the list.

```{r prep_BLIMP_input, echo=5:11, eval=FALSE}
```
Read the input table into `input_orig`. Keep only the `id` and `item` columns. Assign the column names of `input_org` to an object for later use.
```{r prep_BLIMP_input, echo=13:17, eval=FALSE}
```
Count the `NA` cells across all items and persons in the input table. `is.na()` returns `TRUE` for each table cell containing `NA`. Because logical `TRUE` overlays a numerical code of `1`, `sum()` returns the count of `NA` across all cells. `NA_count` prints this count to the console.
```{r prep_BLIMP_input, echo=19:21, eval=FALSE}
```
Recode the `NA` in the input to `999`, a missing value code typically used by BLIMP. Assign the value `999` to the subset of cells within `input_orig` for which the predicate (logical) expression `is.na(input_orig)` returns `TRUE`.
```{r prep_BLIMP_input, echo=22, eval=FALSE}
```
Recall that the structure of the input file is a row for each case, and a column for each item. The model to be processed by BLIMP requires a multi-level (nested) structure, in which the items and responses are nested within each person. The nested rows contain key-value pairs of item-response. Thus each person has the same number of rows as the number of items. Within each set of person-rows, the left-right sequence of columns (item names) in the input file is represented in the `item` column, going down the rows. In the `response` column, the response for each item appears in the same row as the item name. The following snippet accomplishes this transformation, from a wide (or spread) input table, to a tall (or gathered) piped data object.

`tidyr::gather()` transforms the table. In this call, `'item'` and `'response'` name the key and value columns, respectively. Recall that `input_orig` contains only the `id` and the `item` columns. Including `-id` in the `gather()`call drops this column from inclusion in the key-value reformatting. By excluding it in this way, `id` appears in the left-most position in the transformed table, and is "stretched" down the table, creating new duplicate rows for each value of `id`, such that there are the same number of rows for each value of `id` as the number of `item` columns in the input file. In the transformed table, the `item` column contains the `item` names from input file, repeated anew for (nested within) each successive set of `id` rows. The `response` column contains the item responses for each particular pairing of `id` and `item` values.

Here's are examples of the table structure before and after this transformation, for three cases of a four-item test:

###### Before

ID            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

###### After

ID            | Item          | Response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

```{r prep_BLIMP_input, echo=24:25, eval=FALSE}
```
`dplyr::group_by(id)` collects the rows with identical values of `id`, so they can retain the "single row" structure of the input table and be summarized in later procedures. `dplyr::arrange()` sorts the data by `id`. `mutate()` modifies the `item` column in place, extracting the numeric portion of the value of `item` using `stringr::str_sub()`, and coercing it to an integer by wrapping the expression in `as.integer()`.
```{r prep_BLIMP_input, echo=26:28, eval=FALSE}
```
The current data object `input_gathered` has three columns: `id`, `item`, and `response`. We now add a column that provides each item's subscale assignment.

Recall that earlier we created a list:  

* `scale_assign`: this list contains a series of integer ranges (e.g., `1:14`, `15:33`, `34:48`, etc.) that group items by their assigned subscale. For example, the range `1:14` gives the numbers of the 14 items that comprise subscale 1.

We now use `tibble::enframe()` to tranform this list into a data frame and assign it to `input_scale`. The resulting data frame has two columns:

* `name`: the numeric names of the subscales
* `value`: a list-column containing the integer ranges representing the item assignments for each subscale. In a sense, these ranges of items are nested within each value of name (each subscale)

`tidyr::unnest()` operates on the list-column, making each element of each integer range in the list into its own row. The data object now has the same number of rows as the number of items in the test, with each item in the `item` column paired with its subscale name in the `name` column. `dplyr::rename()` renames the `name` column to `scale`.

`dplyr::right_join()` is used to add the `scale` column to `input_gathered`. In this call, `right_join` returns all rows from the RHS data frame, which is the tall table `input_gathered`. The two tables are matched by the `item` column, meaning that each time an `item` number appears, it is paired with its corresponding `scale` number, which appears in the `scale` column. This item-scale correspondence is replicated down the entire height of the tall data object.

```{r prep_BLIMP_input, echo=29:33, eval=FALSE}
```
We now have a four-column data object. `select()` orders the columns in the desired left-right sequence. `mutate()` then adds a fifth column `scale_last`, which assigns the label `1` to the rows containing the last item in each subscale. These rows are labeled with `case_when`, and the labeled rows are those where a predicate returns `TRUE`: _either_ the value of `scale` in the leading row is different `scale != lead(scale)`, _or_ the value of scale in the leading row is missing `is.na(lead(scale))`. The latter condition is needed to capture the last row in the table, where there is no leading row. Finally, `mutate_at` is used to apply a function to an extant column `vars(item)`, the function `as.factor` passed by the formula sign `~`, to coerce `item` to factor as required by downstream code.

```{r prep_BLIMP_input, echo=34:38, eval=FALSE}
```

The model to be imputed by BLIMP requires the item names to be represented as dummy-code columns. To capture subscale assignment, each item is dummy-coded relative to the _last_ item in its subscale. Substantively, this means that each item receives a code of `1` in its particular dummy column, except for the last item in each subscale, which retains a code of `0`.

Below is an example of this dummy-coding scheme, for a six-item test with two subscales: subcale 1, consisting of items 1, 2, and 3, and subscale 2, consisting of items 4, 5, and 6. Each item is represented in its own dummy-code column, `d1` to `d6`. The last items in the two scales are items 3 and 6. Notice how `d3` is coded `0` for item 3, and `d6` is coded `0` for item 6, denoting that these two items are the last items in their respective subscales.

Item          | Scale         | d1            | d2            | d3            | d4            | d5            | d6            
------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | -------------
1             | 1             | 1             | 0             | 0             | 0             | 0             | 0
2             | 1             | 0             | 1             | 0             | 0             | 0             | 0
3             | 1             | 0             | 0             | 0             | 0             | 0             | 0
4             | 2             | 0             | 0             | 0             | 1             | 0             | 0
5             | 2             | 0             | 0             | 0             | 0             | 1             | 0
6             | 2             | 0             | 0             | 0             | 0             | 0             | 0

We can generate the dummy-code columns with the `fastDummies` package. The following snippet assigns `input_scale` to `dum`, to which the new columns will be appended. `fastDummies::dummy_cols()` creates the dummy columns; the variable to be dummy-coded must be coerced to `factor` prior to processing. The argument `select_columns` is used to specify the variable(s) that will be dummy coded. The new dummy columns are named with the format `[variableName]_[variableValue]` (e.g., "item 3" becomes `item_3`)

`mutate_at()` is used to modify the new dummy-code colums so that each item is coded relative to the last item in its scale. `vars(starts_with("item_"))` identifies the scope of `mutate_at()`, i.e., those columns whose names begin with the prefix `item_`. The function to be applied to these columns, via the `~` operator, is `base::replace()`. Its arguments are `.`, indicating that it will operate on the piped data, `scale_last == 1`, indicating that it will replace values in the rows that contain the last item in each subscale, and `0`, the replacement value. `select()` is used to drop the `scale_last` column, which is not needed in the BLIMP input data.
```{r prep_BLIMP_input, echo=40:43, eval=FALSE}
```
Write the input file for BLIMP, stripping the column names `col_names = F` as required. The
input file is written to the top folder of the RStudio project. The BLIMP
script must also be located in this folder (if it is not, a file path must be specified on the BLIMP `DATA` command).
```{r prep_BLIMP_input, echo=45:48, eval=FALSE}
```

<br>

### Prepare data for BLIMP: Single-factor model

The single-factor code is a simplified version of the multi-factor code; the comments and explanations above apply to this code as well.

###### VALID CODE TO RUN

```{r prep_BLIMP_SF_input, eval=FALSE}
suppressMessages(library(here))
suppressMessages(library(tidyverse))
suppressMessages(library(fastDummies))

id <- c('{TOKEN}')
first_item <- c('{TOKEN}')
last_item <- c('{TOKEN}')
file_name <- c('{FILE-NAME}')

input_orig <- suppressMessages(read_csv(here(
  paste0('INPUT-FILES/', file_name, '.csv')
))) %>% 
  select(id, first_item:last_item)
names_input_orig <- names(input_orig)

NA_count <- sum(is.na(input_orig))
NA_count

input_orig[is.na(input_orig)] <- 999

input_gathered <- input_orig %>%
  gather('item','response',-id) %>% 
  group_by(id) %>% 
  arrange(id) %>% 
  mutate(item = as.factor(str_sub(item, 2, 4)))

dum <- input_gathered %>% 
  dummy_cols(select_columns = 'item')

write_csv(dum,
          here(paste0(file_name, '-BLIMP-input.csv')),
          col_names = F
)
```

<br>

### Run BLIMP script on prepped data

This section will deal first with multi-factor input data.

After processing in R, multi-factor input data has the following structure:

* Left-most four columns are Person ID, item number, item response, numerical label for item-scale assignment
* All columns to the right of these four are dummy-code columns, in which each item is dummy-coded relative to the last item in its subscale, such that all items are coded `1` in their dummy columns, except for the last item in each subscale, which is coded `0`.

In a dramatic oversimplification, BLIMP accomplishes the imputation of missing values by estimating a series of multiple regression models on the input data. The estimation is an iterative process, whereby BLIMP posits model parameters, evaluates indices of fit, and adjusts parameters accordingly. Iteration continues for a defined interval. The final regression equations are then used to predict missing values.

The model estimation process must include a burn-in phase: a series of iterations that allow the regression parameters to converge (stabilize). The number of iterations required for burn-in varies by data set and is initially unknown. BLIMP supplies a metric for determining convergence: potential scale reduction (PSR). An acceptable burn-in phase is defined as the minimum number of iterations required to yield a PSR value in the range of 1.05-1.10.

The BLIMP imputation process consists of two steps:

1. __Diagnostic Run__: Data are processed with an initial burn-in interval of 1000 iterations. The output table of PSR values is examined, and if PSR is not reduced to 1.05-1.10 by the last set of iterations, the data are re-run with a longer burn-in interval (e.g. 2000 iterations).  This process continues until an acceptable number of burn-in iterations is determined.

2. __Imputation Run__: Data are processed using the burn-in interval determined in Step 1. Output is an imputed data set with all missing responses estimated.

An identical BLIMP template can be used for Steps 1 and 2. This template consists of the following commands (some include values and arguments that will remain constant in for all input data sets):

```{r BLIMP-multiFactor-template, eval=FALSE}
DATA: {FILE-NAME}.csv
VARIABLES: {TOKEN}
ORDINAL: {TOKEN}
NOMINAL: {TOKEN}
FIXED: {TOKEN}
CLUSTERID: {TOKEN}
MISSING: 999
MODEL: {TOKEN}
SEED: 90291;
BURN: {TOKEN}
ITERATIONS:  1;
OPTIONS: psr;
SAVE: separate = {FILE-NAME}.csv;
```

Steps 1 and 2 are differentiated by activation/deactivation of the `OPTIONS` and `SAVE` lines (commands can be toggled off by beginning the command line with `#`).

###### Step 1
`OPTIONS` is activated to generate the PSR table. `SAVE` is deactivated because no imputed data set is needed for this step.
```{r BLIMP-multiFactor-template1, eval=FALSE}
OPTIONS: psr;
# SAVE: separate = {FILE-NAME}.csv;
```
###### Step 2
`OPTIONS` is deactivated because the burn-in interval has been determined by Step 1, so there is no need to examine the PSR table. `SAVE` is activated in order to generated an imputed data set with missing values estimated.
```{r BLIMP-multiFactor-template2, eval=FALSE}
# OPTIONS: psr;
SAVE: separate = {FILE-NAME}.csv;
```

Here is an example script for a multi-factor test with 114 items and eight subscales. The example has a typical set of token substitutions.

###### THIS CODE CAN BE RUN IN BLIMP STUDIO
```{r BLIMP, eval=FALSE}
DATA: BLIMP-input.csv;
VARIABLES: id itemnum response scalenum s1d1-s1d14 s2d1-s2d16 s3d1-s3d12 s4d1-s4d23 s5d1-s5d8 s6d1-s6d14 s7d1-s7d16 s8d1-s8d11;
ORDINAL: response;
NOMINAL: scalenum;
FIXED: scalenum s1d1-s1d13 s2d1-s2d15 s3d1-s3d11 s4d1-s4d22 s5d1-s5d7 s6d1-s6d13 s7d1-s7d15 s8d1-s8d10;
CLUSTERID: id;
MISSING: 999;
MODEL: response ~ scalenum s1d1-s1d13 s2d1-s2d15 s3d1-s3d11 s4d1-s4d22 s5d1-s5d7 s6d1-s6d13 s7d1-s7d15 s8d1-s8d10 | scalenum;
SEED: 90291;
BURN: 2000; 
ITERATIONS: 1;
OPTIONS: psr;
SAVE: separate = BLIMP-ouput.csv;
```

<br>

###### COMMENTED SNIPPETS
In order to run as written, the `DATA` command must name a file located in the same folder/directory as the BLIMP script. If the input file is located elsewhere, you would need to specify a file path on `DATA`. Because the BLIMP input file has no column names, the `VARIABLE` command is used to name the columns. The first four names (`id`, `itemnum`, `response`, `scalenum`) are applied, respectively, to the four left-most columns on the input file: person ID, item number, item response, and item-scale assignment. The names to the right of `scalenum` are applied to the dummy code columns (d) for each of the eight scales (s). Thus, for example, `s2d16` names the dummy code column for the 16th item of the second scale. The expression `s1d1-s1d14`, for example, is expanded by BLIMP to yield 14 numerically sequenced names (i.e., `s1d1`, `s1d2`, `s1d3`, etc.)
```{r BLIMP, echo=1:2, eval=FALSE}
```
`ORDINAL` and `NOMINAL` are used to designate categorical variables. In the input data file, `response` refers to a 1-2-3-4 item response format; i.e., ordered categories representing increasing frequency of a behavior. On the `NOMINAL` line, `scalenum` represents a non-ordered categorical variable. Variables on the `NOMINAL` line are automatically recoded into dummy codes at imputation.
```{r BLIMP, echo=3:4, eval=FALSE}
```
`FIXED` is used to designate variables that will not have their means and variances estimated during computation. Fixing variables in this way increases computational efficiency. In this example, the means and variances of the item-scale assigment variable and the dummy code columns are not of substantive interest; thus, there is no reason to estimate these parameters. 

`CLUSTERID` designates a multilevel (nested) design. In this example, on the input file, values of `itemnum` are nested within each value of `id`.
```{r BLIMP, echo=5:6, eval=FALSE}
```
`MISSING` identifies the missing data code (the code `999` was created in the R script). `MODEL` specifies the regression model that will be used for imputation. On this `MODEL` line, `response` is regressed on the dummy-coded item-scale assignment variables (created by listing `scalenum` on the `NOMINAL` command line) and the dummy-coded item variables (represented as columns in the input .csv). 

The expression `| scalenum` allows the model to estimate individual differences for each subscale score. This is a flexible model in which the only assumption is that within-subscale factor loadings (thetas) are identical. Items within scales can have unique means, and thetas can vary across scales and between persons.
```{r BLIMP, echo=7:8, eval=FALSE}
```

#######START HERE

Set `SEED` so that output is identical on each run. `BURN` runs 1000 iterations before saving the first imputed data set, to allow parameter estimates to stabilize. Number of `BURN` iterations needs to be recalibrated for each new data set by examining psr, plots. `ITERATIONS` controls how many iterations are run after completing burn-in, set this to at least 1000 to ensure stable parameter estimates. 
```{r BLIMP, echo=9:11, eval=FALSE}
```
`OPTIONS` requests parameter estimates for the analysis output. `CHAINS` is toggled off here, but it can be turned on for parallel (and potentially faster) processing. In this example, four chains would then be run, resulting in four imputed data sets. `SAVE: separate` indicates that imputations will be saved in separate files (this script is set up to yield only one imputed data set). The asterisk `*` is required so that BLIMP can distinguish multiple separate imputations with a numerical suffix.
```{r BLIMP, echo=12:14, eval=FALSE}
```

<br>

### Reformat imputed data set for downstream analysis

The imputed data set is in a tall (gathered) format with the dummy code columns. This code restores the data to its original input structure and writes an output file.

###### VALID CODE TO RUN

```{r reformat_impute, eval=FALSE}
temp1 <- suppressMessages(
  read_csv(
    (here('model4imp1.csv')), col_names = F)[1:3])
names(temp1) <- c('id', 'item', 'response')
temp2 <- temp1 %>% 
  spread(item, response) 
names(temp2) <- names_input_orig

NA_count <- sum(temp2 == 999)
NA_count

write_csv(temp2, here(
  paste0(
    'OUTPUT-FILES/',
    file_name,
    '-noMiss.csv'
  )
))
```

<br>

###### COMMENTED SNIPPETS
The imputed data file `model4imp1.csv` has no column names, so `read_csv` must be called with the `col_names = F` argument (otherwise, the first row of data will be used as column names). Subsetting `[1:3]` is used to read in only the three left-most columns. Column names are applied from a character vector using `names()`.
```{r reformat_impute, echo=1:4, eval=FALSE}
```
The data object `temp1` is a tall table in which `item`s and their associated `response`s are nested within each person `id` number, as in the following example:

###### Before

id            | item          | response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

The application of `tidyr::spread()` collapes the muliple rows of `id` into a single row. The call identifies the key-value pair: `item` (key) and `response` (value). The values of `item` are used to name new columns going wide across the table. The values of `response` become the cell entries at the intersection of each row `id` and column `item`. The table appears as in the example below:

###### After

id            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

Thus, `spread()` restores the imputed data set to the same format as original input (which contained missing data). `names` is used to reapply the column names of the original input.

The remainder of this code chunk verifies that the count of `NA` (now coded as `999`) is 0, and writes the final output to .csv.

```{r reformat_impute, echo=5:18, eval=FALSE}
```

<br>

### Annotated BLIMP output
For all output tables, the entries in the mean/StdDev columns can be regarded as Bayesian point estimtes and standard errors. The model allows each person to have a latent scale (subscale) score on each subscale/factor/dimension. These latent scores are approximately scaled as z-scores.

#### Variances Section

* Lines that begin with L2 and have only a single quantity in the `Variances` column (e.g., `L2 Intercept (i)`, `L2 scalenum#2`) are _variances_ of a particular scale score. `L2 Intercept (i)` is the variance of scale #1. 
* Lines that have two elements (e.g., `L2 scalenum#3, scalenum#2`) are the _covariances_ between two scale scores.

```{r BLIMP_output_var, eval=FALSE}
ANALYSIS MODEL ESTIMATES:


Missing outcome: response   

                                --------------------------------------------------------
Parameters                      |   Mean   |  Median  |  StdDev  |Lower 2.5 |Upper 97.5|
                                --------------------------------------------------------
Variances:                      |          |          |          |          |          |
  L2 Intercept (i)              |     0.729|     0.707|     0.062|     0.675|     0.860|
  L2 (i), scalenum#2            |    -0.365|    -0.344|     0.059|    -0.474|    -0.295|
  L2 scalenum#2                 |     0.678|     0.685|     0.062|     0.602|     0.794|
  L2 (i), scalenum#3            |    -0.400|    -0.378|     0.064|    -0.523|    -0.333|
  L2 scalenum#3, scalenum#2     |     0.670|     0.654|     0.069|     0.595|     0.820|
  L2 scalenum#3                 |     0.836|     0.803|     0.088|     0.759|     1.039|
  L2 (i), scalenum#4            |    -0.371|    -0.332|     0.066|    -0.502|    -0.307|
  L2 scalenum#4, scalenum#2     |     0.538|     0.530|     0.059|     0.469|     0.667|
  L2 scalenum#4, scalenum#3     |     0.647|     0.615|     0.073|     0.592|     0.827|
  L2 scalenum#4                 |     0.704|     0.690|     0.061|     0.642|     0.840|
  
  [OUTPUT TRUNCATED FOR BREVITY]
  
  Residual Var.                 |     1.000|     1.000|     0.000|     1.000|     1.000|
                                |----------|----------|----------|----------|----------|
```

#### Coefficients Section

* `Intercept`: grand mean of latent scale #1.
* `scalenum`: dummy codes reflecting mean differences of the listed scale from scale #1 (expressed roughly on a z-score scale). 
    + In this example, all scales have lower means than scale #1, except for scale #3. Confidence intervals in the right-most columns tell whether a scale's mean differs significantly from the mean of scale #1.
* `s1d1` et al.: dummy codes that capture differences among items with respect to difficulty.
    + Coefficents give mean difference of each item relative to last item on that scale.

```{r BLIMP_output_coeff, eval=FALSE}
Coefficients:                   |          |          |          |          |          |
  Intercept                     |     0.136|     0.119|     0.053|     0.078|     0.231|
  scalenum#2                    |    -0.507|    -0.498|     0.056|    -0.623|    -0.421|
  scalenum#3                    |     0.295|     0.321|     0.077|     0.152|     0.374|
  scalenum#4                    |    -1.085|    -1.057|     0.083|    -1.255|    -1.003|
  scalenum#5                    |    -1.979|    -1.968|     0.099|    -2.131|    -1.864|
  scalenum#6                    |    -1.784|    -1.798|     0.056|    -1.854|    -1.707|
  scalenum#7                    |    -1.163|    -1.162|     0.047|    -1.236|    -1.077|
  scalenum#8                    |    -2.004|    -1.987|     0.082|    -2.169|    -1.910|
  s1d1                          |     0.003|     0.016|     0.079|    -0.122|     0.098|
  s1d2                          |     0.046|     0.062|     0.069|    -0.055|     0.130|
  s1d3                          |     0.307|     0.305|     0.057|     0.203|     0.409|

      [OUTPUT TRUNCATED FOR BREVITY]

  s8d7                          |     0.660|     0.637|     0.073|     0.536|     0.765|
  s8d8                          |     0.775|     0.761|     0.045|     0.704|     0.838|
  s8d9                          |     1.289|     1.282|     0.062|     1.190|     1.391|
  s8d10                         |     1.302|     1.322|     0.053|     1.183|     1.356|
                                |----------|----------|----------|----------|----------|
```

#### Threshold Section

* `Tau`: set of z-score cutpoints that slice underlying latent normal scores into four discrete response options (for four-point item rating scale). This is not a direct analog to polytomous response thresholds in IRT, because the latter are available on an item-by-item basis, whereas these Tau paramaters represent a shared aspect of all items in the BLIMP analysis.

```{r BLIMP_output_tau, eval=FALSE}
Thresholds:                     |          |          |          |          |          |
  Tau 1                         |     0.000|     0.000|     0.000|     0.000|     0.000|
  Tau 2                         |     1.598|     1.598|     0.005|     1.590|     1.606|
  Tau 3                         |     2.555|     2.556|     0.014|     2.532|     2.572|
                                |----------|----------|----------|----------|----------|
```


