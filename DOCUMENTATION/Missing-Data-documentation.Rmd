---
title: "Impute Missing Data with BLIMP"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview  

In this method, an input file with missing data is prepped in R and then run through BLIMP, which estimates a regression model and generates an output data set in which all missing cells are replaced with imputed values.

This procedure assumes that data are missing from the input file due to the missing at random (MAR) mechanism. A discussion of missing data theory is beyond the scope of this document, and such knowledge is not needed to use the method described here. Enders (2010) provides a thorough treament of missing data methods and theory.

``` 
Enders, C. K. (2010). Applied Missing Data Analysis. New York: Guilford. 
``` 

The code assumes a typical RStudio project folder hierarchy, with `INPUT-FILES` and `OUTPUT-FILES` folders at the first level.

### Prepare data for BLIMP

Input data should formatted with cases in rows and test items in columns, with a person ID column on the far left. On the input file, items should be renamed as follows: i001, i002, i003, etc. If the input file includes items from different scales/subtests, with different prefixes in their names, those prefixes must be dropped and the items renamed with the `i001` consecutive nomenclature.

Once data are in this format, run the next block of code to restructure the input for BLIMP.

Here and throughout, certain token markers are employed to designate user-input values that vary by project:  

* `{TOKEN}`: any value or series of values  
* `{FILE-PATH}`  
* `{FILE-NAME}` 

###### VALID CODE TO RUN

```{r prep_BLIMP_input, eval=FALSE}
suppressMessages(library(here))
suppressMessages(library(tidyverse))

id <- c('{TOKEN}')
first_item <- c('{TOKEN}')
last_item <- c('{TOKEN}')
file_name <- c('{FILE-NAME}')

input_orig <- suppressMessages(read_csv(here(
  paste0('INPUT-FILES/', file_name, '.csv')
))) %>% 
  select(id, first_item:last_item)
names_input_orig <- names(input_orig)

NA_count <- sum(is.na(input_orig))
NA_count

input_orig[is.na(input_orig)] <- 999

input_gathered <- input_orig %>%
  gather('item','response',-id) %>% 
  group_by(id) %>% 
  arrange(id) %>% 
  mutate(item = as.factor(str_sub(item, 2, 4)))

write_csv(input_gathered,
          here(paste0(file_name, '-BLIMP-input.csv')),
          col_names = F
)
```

###### COMMENTED SNIPPETS
The snippet below contains user-programmable input paramaters. Here are the token substitutions:

* `id`: name of person ID column
* `first_item`: name of column holding first item
* `last_item`: name of column holding last item
* `file_name`: _prefix_ of input file name (i.e., stripped of `.csv`)
```{r prep_BLIMP_input, echo=4:8, eval=FALSE}
```
Read `readr::read_csv()` the input table into `input_orig`. Keep `dplyr::select()` only the `id` and `item` columns (the expression `first_item:last_item` captures all item columns, from first through last). Assign the column names of `input_org` to an object for later use.
```{r prep_BLIMP_input, echo=9:13, eval=FALSE}
```
When data are read into R, missing values are coded `NA` (not available). The next snippet counts the `NA` cells across all items and persons in the input table. `is.na()` returns `TRUE` for each table cell containing `NA`. Because logical `TRUE` is equivalent to a numerical code of `1`, `sum()` returns the count of `NA` across all cells. `NA_count` prints this count to the console.
```{r prep_BLIMP_input, echo=15:16, eval=FALSE}
```
Recode the `NA` in the input to `999`, a missing value code typically used by BLIMP. Assign the value `999` to the subset of cells within `input_orig` for which the predicate (logical) expression `is.na(input_orig)` returns `TRUE`.
```{r prep_BLIMP_input, echo=18, eval=FALSE}
```
Recall that the structure of the input file is a row for each case, and a column for each item. The model to be processed by BLIMP requires a multi-level (nested) structure, in which the items and responses are nested within each person. The nested rows contain key-value pairs of item-response. In the multi-level structure, each person has the same number of rows as the number of items in the input file. 

Within the set of rows that share an identical value for `id`, the left-right sequence of columns (item names) in the input file is represented in the `item` column, going down the rows. In the `response` column, the response for each item appears in the same row as the item name. The following snippet accomplishes this transformation, from a wide (or spread) input table, to a tall (or gathered) piped data object.

`tidyr::gather()` transforms the table. In this call of `gather()`, `'item'` and `'response'` name the key and value columns, respectively. Recall that `input_orig` contains only the `id` and the `item` columns. Including `-id` in the `gather()`call drops this column from inclusion in the key-value reformatting. By excluding it in this way, `id` appears in the left-most position in the transformed table, and is "stretched" down the table, creating new duplicate rows for each value of `id`, such that there are the same number of rows for each value of `id` as the number of `item` columns in the input file. In the transformed table, the `item` column contains the `item` names from the input file, repeated anew for (nested within) each successive set of `id` rows. The `response` column contains the item responses for each particular pairing of `id` and `item` values.

Here are examples of the table structure before and after this transformation, for three cases of a four-item test:

###### Before

ID            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

###### After

ID            | Item          | Response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

```{r prep_BLIMP_input, echo=20:21, eval=FALSE}
```
`dplyr::group_by(id)` collects the rows with identical values of `id`, so they can retain the "single row" structure of the input table and be summarized in later procedures. `dplyr::arrange()` sorts the data by `id`. `dplyr::mutate()` modifies the `item` column in place, extracting the numeric portion of the value of `item` using `stringr::str_sub()`, and coercing it to a factor by wrapping the expression in `as.factor()`.
```{r prep_BLIMP_input, echo=22:24, eval=FALSE}
```
Write the input file for BLIMP, stripping the column names `col_names = F` as required by BLIMP. The input file is written to the top folder of the RStudio project. The BLIMP
script must also be located in this folder (if it is not, a file path must be specified on the BLIMP `DATA` command).
```{r prep_BLIMP_input, echo=26:29, eval=FALSE}
```

<br>

### Run BLIMP script on prepped data

After processing in R, the BLIMP input file has the following structure:

* Three columns: Person ID, item number, item response
* The structure is multi-level, in which item-response pairs are nested within each value of Person ID
* No column (variable) names

In simple terms, BLIMP accomplishes the imputation of missing values by estimating a series of multiple regression models on the input data. The estimation is an iterative process, whereby BLIMP posits model parameters, evaluates indices of fit, and adjusts parameters accordingly. Iteration continues for a defined interval. The final regression equations are then used to impute missing values.

The model estimation process must include a burn-in phase: a series of iterations that allow the regression parameters to converge (stabilize). The number of iterations required for burn-in varies by data set and is initially unknown. BLIMP supplies a metric for determining convergence: potential scale reduction (PSR). An acceptable burn-in interval is defined as the minimum number of iterations required to yield a PSR value in the range of 1.05-1.10.

The BLIMP imputation process consists of two steps:

1. __Diagnostic Run__: Data are processed with an initial burn-in interval of 1000 iterations. The output table of PSR values is examined, and if PSR is not reduced to 1.05-1.10 by the last set of iterations, the data are re-run with a longer burn-in interval (e.g. 2000 iterations).  This process continues until an acceptable number of burn-in iterations is determined.

2. __Imputation Run__: Data are processed using the burn-in interval determined in Step 1. The saved output data set has all missing responses replaced with imputed values.

An identical BLIMP template script can be used for Steps 1 and 2. This template consists of the following commands (some include values and arguments that remain constant for all input data sets):

```{r BLIMP-multiFactor-template, eval=FALSE}
DATA: {FILE-NAME}.csv
VARIABLES: {TOKEN}
ORDINAL: {TOKEN}
NOMINAL: {TOKEN}
FIXED: {TOKEN}
CLUSTERID: {TOKEN}
MISSING: 999
MODEL: {TOKEN}
SEED: 90291;
BURN: {TOKEN}
ITERATIONS:  1;
OPTIONS: psr;
SAVE: separate = {FILE-NAME}.csv;
```

Steps 1 and 2 are differentiated by activation/deactivation of the `OPTIONS` and `SAVE` lines (commands can be toggled off by beginning the command line with `#`).

###### Step 1: Diagnostic Run
`OPTIONS` is activated to generate the PSR table. `SAVE` is deactivated because no imputed data set is needed for this step.
```{r BLIMP-multiFactor-template1, eval=FALSE}
OPTIONS: psr;
# SAVE: separate = {FILE-NAME}.csv;
```
###### Step 2: Imputation Run
`OPTIONS` is deactivated because the burn-in interval has been determined by Step 1, so there is no need to view the PSR table. `SAVE` is activated in order to generate an imputed data set with missing values estimated.
```{r BLIMP-multiFactor-template2, eval=FALSE}
# OPTIONS: psr;
SAVE: separate = {FILE-NAME}.csv;
```

Below is an example BLIMP script based on this template. The example has a typical set of token substitutions. The file names are generic - any suitable names can be substituted.

###### THIS CODE CAN BE RUN IN BLIMP STUDIO
```{r BLIMP_MF, eval=FALSE}
DATA: BLIMP-input.csv;
VARIABLES: id itemnum response;
ORDINAL: response;
NOMINAL: itemnum;
FIXED: itemnum;
CLUSTERID: id;
MISSING: 999;
MODEL: response ~ itemnum;
SEED: 90291;
BURN: 2000; 
ITERATIONS: 1;
OPTIONS: psr;
SAVE: separate = BLIMP-output*.csv;
```

###### COMMENTED SNIPPETS
In order to run as written, the `DATA` command must name a file located in the same folder/directory as the BLIMP script. If the input file is located elsewhere, a file path must be specified on `DATA`. Because the input date file has no column names, the `VARIABLES` command is used to name the three columns in the input file: `id`, `itemnum`, `response`. The number of names on the `VARIABLES` line must be identical to the number of columns in the input file.
```{r BLIMP_MF, echo=1:2, eval=FALSE}
```
`ORDINAL` and `NOMINAL` are used to designate ordered and non-ordered categorical variables. Here, `response` is listed on the `ORDINAL` line. In the input data file, `response` refers to a 1-2-3-4 item response format; i.e., ordered categories representing increasing frequency of a behavior. On the `NOMINAL` line, `itemnum` represents a non-ordered categorical variable. Variables on the `NOMINAL` line are automatically recoded into dummy codes at imputation.
```{r BLIMP_MF, echo=3:4, eval=FALSE}
```
`FIXED` is used to designate variables that will not have their means and variances estimated during computation. Fixing variables in this way increases computational efficiency. In this example, the means and variances of the nominal item number variable are not of substantive interest; thus, there is no reason to estimate these parameters. 

`CLUSTERID` designates a multilevel (nested) design. In this example, on the input file, key-value pairs of `itemnum` and `response` are nested within each value of `id`.
```{r BLIMP_MF, echo=5:6, eval=FALSE}
```
`MISSING` identifies the missing data code (`999` was coded in the input file by the R script). `MODEL` specifies the regression model that will be used for imputation. On this `MODEL` line, `response` is regressed on the dummy-coded item-number variable (dummy codes were created by listing `itemnum` on the `NOMINAL` command line). 
```{r BLIMP_MF, echo=7:8, eval=FALSE}
```
`SEED` sets BLIMP's random-number generator so that the final output is identical each time the script is run. `BURN` specifies the number of iterations that are run prior to saving the first imputed data set. As described above, `BURN` is initially set at 1000 for the diagnostic run, and this value is adjusted as needed to until PSR reaches the acceptable threshold. This adjusted value is then used for the imputation run.

`ITERATIONS` controls how many iterations are run after completing burn-in. For the present purpose, which is generating an imputed data set, `ITERATIONS` is set to 1. If the project required regression parameters as explicit output, `ITERATIONS` would be set to at least 1000 to ensure stable estimates.
```{r BLIMP_MF, echo=9:11, eval=FALSE}
```
`OPTIONS` and `SAVE` are the two BLIMP commands that are set differently for the diagnostic and imputation runs. For the diagnostic run, `SAVE` is toggled off, and `OPTIONS` is run to generate the PSR table that is used to determine the optimal burn-in interval. For the imputation run, `OPTIONS` is toggled off, and `SAVE` is run to save the imputed data set. The `separate` argument indicates that each imputed data set is to be saved as a separate file (as opposed to being stacked on top of one another in a single file). The file name on `SAVE` requires an `*`, which allows BLIMP to append numerical suffixes on multiple imputed data files. (`*` is always used, even in this example, which yields only a single imputed data set)
```{r BLIMP_MF, echo=12:13, eval=FALSE}
```

<br>

### Reformat imputed data set for downstream analysis

The imputed data set is in a tall (gathered) format with the dummy code columns. This code restores the data to its original input structure and writes an output file.

###### VALID CODE TO RUN

```{r reformat_impute, eval=FALSE}
temp1 <- suppressMessages(
  read_csv(
    (here('BLIMP-output1.csv')), col_names = F))
names(temp1) <- c('id', 'item', 'response')
temp2 <- temp1 %>% 
  spread(item, response) 
names(temp2) <- names_input_orig

NA_count <- sum(temp2 == 999)
NA_count

write_csv(temp2, here(
  paste0(
    'OUTPUT-FILES/',
    file_name,
    '-noMiss.csv'
  )
))
```

<br>

###### COMMENTED SNIPPETS
The imputed data file `BLIMP-output1.csv` has no column names, so `read_csv` must be called with the `col_names = F` argument (otherwise, the first row of data will be used as column names). Column names are applied from a character vector using `base::names()`.
```{r reformat_impute, echo=1:4, eval=FALSE}
```
The data object `temp1` is a tall table in which `item`s and their associated `response`s are nested within each person `id` number, as in the following example:

###### Before

id            | item          | response
------------- | ------------- | -------------
1001          | i01           | 2
1001          | i02           | 2
1001          | i03           | 3
1001          | i04           | 1
1002          | i01           | 4
1002          | i02           | 1
1002          | i03           | 4
1002          | i04           | 2
1003          | i01           | 3
1003          | i02           | 2
1003          | i03           | 3
1003          | i04           | 4

The application of `tidyr::spread()` collapses the muliple rows of `id` into a single row. The call identifies the key-value pair: `item` (key) and `response` (value). The values of `item` are used to name new columns going left-to-right across the table. The values of `response` become the cell entries at the intersection of each `id` row and `item` column. The transformed table appears as in the example below:

###### After

id            | i01           | i02           | i03           | i04
------------- | ------------- | ------------- | ------------- | -------------
1001          | 2             | 2             | 3             | 1
1002          | 4             | 1             | 4             | 2
1003          | 3             | 2             | 3             | 4

Thus, `spread()` restores the imputed data set to the same format as original input (which contained missing data). In this sense, `spread()` is the inverse of `gather()`, which was used in earlier code to transform the original input into the multi-level format required by BLIMP.

In the remainder of this code chunk, `names()` is used to reapply the column names of the original input. The code verifies that the count of `NA` (now coded as `999`) is 0, and writes the final output to .csv. The output has no missing data and is thus ready for downstream analysis.

```{r reformat_impute, echo=5:18, eval=FALSE}
```
